1.volatile 
------
    在多处理器下，为了保证各个处理器的缓存是一致的，就会实现**缓存一致性**协议，**每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期**了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。
    
    1.1 在生成汇编代码时会对volatile修饰的共享变量进行写操作的时候会多出Lock前缀的指令，Lock前缀的指令会引起处理器缓存写回内存。
    1.2 一个处理器的缓存回写到内存会导致其他处理器的缓存失效。
    1.3 当处理器发现本地缓存失效后，就会从内存中重读该变量数据，即可以获取当前最新值。
      volatile变量通过这样的机制就使得每个线程都能获得该变量的最新值。
      
    volatile：jdk提供的一种轻量级的同步机制，保证不同线程间对共享变量操作的可见性，阻止编译器和处理器的指令重排(通过添加JMM内存屏障)，但是不保证原子性
      
 2.CAS
 ------
    compare and swap保证变量的原子性操作
    CAS机制中，使用三个操作数：内存地址V，旧的预期值A，要修改的新值B
    更新一个变量的时候，只有当旧的预期值A和内存地址V中的实际值相同时，才会将内存地址V对应的值修改为B
    底层实现：利用unsafe提供的原子性操作方法，unsafe是底层硬件级的操作
    
    CAS通过自旋实现，1.获取当前值2.当前值+1，计算出目标值3.进行CAS操作，成功就跳出循环，失败就重复上述步骤
    缺点：CPU开销过大，不能保证代码块的原子性，ABA问题
    ABA问题：线程1：A->B
             线程2：阻塞
             线程3：B->A
             线程2：阻塞完成，A->B
             
    ABA问题解决：值+版本号
                例如：AtomicStampedReference类实现了带版本号的CAS机制

3.公平锁+非公平锁
------
    公平锁：指多个线程按照申请锁的顺序来获取锁，先来后到，早到早得
    非公平锁：指多个线程获取锁的顺序并非按照申请锁的顺序，有可能后申请的线程比先申请的线程要更早的获取到锁。
    非公平锁的优点在于吞吐量比公平锁大
    Synchronized锁是一种非公平锁
    ReentrantLock锁可以根据析构函数的参数决定是公平锁还是非公平锁。
  
4.可重入锁  
------
    作用是防止死锁  
    线程获取锁后可以重复执行锁区域。Java提供的锁都是可重入锁。不可重入锁非常容易导致死锁。  
5.共享锁
------
    线程可以同时获取锁。ReentrantReadWriteLock对于读锁是共享的，在读多写少的情况下使用共享锁会非常高效。  
6.排它锁
------
    多线程不可同时获取的锁。与共享锁对立。与可重入锁不矛盾可以是并存属性。  
7.偏向锁  
------
    一段同步代码一直被一个线程所访问，那么该线程会自动获取所。降低获取锁的代价，类似于乐观锁。  
8.轻量级锁  
------
    当锁是偏向锁的时候，被另一个线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的方式尝试获取锁，不会阻塞，提高性能。  
9.重量级锁  
------
    当锁是轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁会膨胀为重量级锁，重量级锁会让其他申请的线程进入阻>>塞， 性能降低。  
10.分段锁  
------
    分段锁是一种锁思想，对数据分段加锁提高并发效率，比如jdk8之前的ConcurrentHashMap(采用分段锁+数组+链表)，jdk8之后CAS+Synchronized。  
    当需要PUT元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道它要放在哪一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，  
    就实现了真正的并行插入。  
    分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅对数组中的一项进行加锁操作。  
11.自旋锁  
------
    是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU  
12.CountDownLatch
------
    CountDownLatch是一个同步工具类，用来协调多个线程之间的同步，或者说多个线程之间的通信。
    CountDownLatch能够使一个线程在等待另外一些线程完成各自的工作后，再继续执行。使用一个计数器实现，计数器初始值为线程的数量，当一个线程完成自己的任务后，计数器的值就会减1，当计数器的值为0时，表示所有的线程都已经完成一些任务，然后在CountDownLatch上等待的线程就可以恢复执行接下来的任务。
    CountDownLatch的典型用法：某一线程开始运行前等待n个线程执行完毕。将CountDownLatch的计数器初始化为new CountDownLatch(n),每当一个线程执行完毕，就将计数器减1，countDownLatch.countDown(),当计数器的值变成0时，在CountDownLatch上await()的线程就会被唤醒。
    CountDownLatch的应用场景就是：启动一个服务的时候，主线程需要等待多个组件加载完毕，再继续执行。
    缺点：
        CountDownLatch是一次性的，初始化的计数器的值，只能初始化一次，不能重复利用。
 13.阻塞队列
 ------
    BlockingQueue
    阻塞队列区别于普通队列的就在于阻塞：当线程想要消费队列中的数据时，如果队列为空，那么线程将会被阻塞挂起，直到队列中出现了数据。
    阻塞队列分为先进先出和后进先出两种策略。
    BlockingQueue的核心方法：

　　1.放入数据

　　　　（1）offer(anObject):表示如果可能的话,将anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则返回false.（本方法不阻塞当前执行方法

 的线程）；　　　　　　 
     　　（2）offer(E o, long timeout, TimeUnit unit)：可以设定等待的时间，如果在指定的时间内，还不能往队列中加入BlockingQueue，则返回失败。

　　　　（3）put(anObject):把anObject加到BlockingQueue里,如果BlockQueue没有空间,则调用此方法的线程被阻断直到BlockingQueue里面有空间再继续.

　　2. 获取数据

　　　　（1）poll(time):取走BlockingQueue里排在首位的对象,若不能立即取出,则可以等time参数规定的时间,取不到时返回null;

　　　　（2）poll(long timeout, TimeUnit unit)：从BlockingQueue取出一个队首的对象，如果在指定时间内，队列一旦有数据可取，则立即返回队列中的数据。否则知道时间超时还没有数据可取，返回失败。
　　　　（3）take():取走BlockingQueue里排在首位的对象,若BlockingQueue为空,阻断进入等待状态直到BlockingQueue有新的数据被加入; 

　　　　（4）drainTo():一次性从BlockingQueue获取所有可用的数据对象（还可以指定获取数据的个数），通过该方法，可以提升获取数据效率；不需要多次分批加锁或释放锁。

　  3. BlockingQueue的实现
          3.1 ArrayBlockingQueue基于数组的阻塞队列实现，在ArrayBlockingQueue内部，维护了一个定长数组，以便缓存队列中的数据对象，这是一个常用的阻塞队列，除了一个定长数组外，ArrayBlockingQueue内部还保存着两个整形变量，分别标识着队列的头部和尾部在数组中的位置。    
              ArrayBlockingQueue在生产者放入数据和消费者获取数据，都是共用同一个锁对象，由此也意味着两者无法真正并行运行，这点尤其不同于LinkedBlockingQueue；按照实现原理来分析，ArrayBlockingQueue完全可以采用分离锁，从而实现生产者和消费者操作的完全并行运行。Doug Lea之所以没这样去做，也许是因为ArrayBlockingQueue的数据写入和获取操作已经足够轻巧，以至于引入独立的锁机制，除了给代码带来额外的复杂性外，其在性能上完全占不到任何便宜。 ArrayBlockingQueue和LinkedBlockingQueue间还有一个明显的不同之处在于，前者在插入或删除元素时不会产生或销毁任何额外的对象实例，而后者则会生成一个额外的Node对象。这在长时间内需要高效并发地处理大批量数据的系统中，其对于GC的影响还是存在一定的区别。而在创建ArrayBlockingQueue时，我们还可以控制对象的内部锁是否采用公平锁，默认采用非公平锁  
           3.2 LinkedBlockingQueue基于链表的阻塞队列，同ArrayListBlockingQueue类似，其内部也维持着一个数据缓冲队列（该队列由一个链表构成），当生产者往队列中放入一个数据时，队列会从生产者手中获取数据，并缓存在队列内部，而生产者立即返回；只有当队列缓冲区达到最大值缓存容量时（LinkedBlockingQueue可以通过构造函数指定该值），才会阻塞生产者队列，直到消费者从队列中消费掉一份数据，生产者线程会被唤醒，反之对于消费者这端的处理也基于同样的原理。而LinkedBlockingQueue之所以能够高效的处理并发数据，还因为其对于生产者端和消费者端分别采用了独立的锁来控制数据同步，这也意味着在高并发的情况下生产者和消费者可以并行地操作队列中的数据，以此来提高整个队列的并发性能。    
　　          作为开发者，我们需要注意的是，如果构造一个LinkedBlockingQueue对象，而没有指定其容量大小，LinkedBlockingQueue会默认一个类似无限大小的容量（Integer.MAX_VALUE），这样的话，如果生产者的速度一旦大于消费者的速度，也许还没有等到队列满阻塞产生，系统内存就有可能已被消耗殆尽了。
           3.3 DelayQueue：DelayQueue中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素。  
               DelayQueue是一个没有大小限制的队列，因此往队列中插入数据的操作（生产者）永远不会被阻塞，而只有获取数据的操作（消费者）才会被阻塞。 
           3.4 PriorityBlockingQueue：基于优先级的阻塞队列（优先级的判断通过构造函数传入的Compator对象来决定），但需要注意的是PriorityBlockingQueue并不会阻塞数据生产者，而只会在没有可消费的数据时，阻塞数据的消费者。因此使用的时候要特别注意，生产者生产数据的速度绝对不能快于消费者消费数据的速度，否则时间一长，会最终耗尽所有的可用堆内存空间。在实现PriorityBlockingQueue时，内部控制线程同步的锁采用的是公平锁。  
           3.5 SynchronousQueue 一种无缓冲的等待队列，类似于无中介的直接交易，有点像原始社会中的生产者和消费者，生产者拿着产品去集市销售给产品的最终消费者，而消费者必须亲自去集市找到所要商品的直接生产者，如果一方没有找到合适的目标，那么对不起，大家都在集市等待。相对于有缓冲的BlockingQueue来说，少了一个中间经销商的环节（缓冲区），如果有经销商，生产者直接把产品批发给经销商，而无需在意经销商最终会将这些产品卖给那些消费者，由于经销商可以库存一部分商品，因此相对于直接交易模式，总体来说采用中间经销商的模式会吞吐量高一些（可以批量买卖）；但另一方面，又因为经销商的引入，使得产品从生产者到消费者中间增加了额外的交易环节，单个产品的及时响应性能可能会降低。  

　　声明一个SynchronousQueue有两种不同的方式，它们之间有着不太一样的行为。公平模式和非公平模式的区别:  

　　如果采用公平模式：SynchronousQueue会采用公平锁，并配合一个FIFO队列来阻塞多余的生产者和消费者，从而体系整体的公平策略；  

　　但如果是非公平模式（SynchronousQueue默认）：SynchronousQueue采用非公平锁，同时配合一个LIFO队列来管理多余的生产者和消费者，而后一种模式，如果生产者和消费者的处理速度有差距，则很容易出现饥渴的情况，即可能有某些生产者或者是消费者的数据永远都得不到处理。  
